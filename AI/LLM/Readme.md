LLM is computer program that can understand, remember the things and then generate human language. It is trained on large amounts of text data to learn patterns, grammar, and context, allowing it to perform tasks like translation, summarization, and conversation.

GPT - Generative Pre-trained Transformer is a type of LLM developed by OpenAI. It uses a transformer architecture to process and generate text, making it capable of understanding context and generating coherent responses.

All the AI models works on transformer architecture model.

What transformer does
Transformer can convert any input to particular output. It can take any input and convert it to a particular output. For example, it can take a sentence in one language and convert it to another language, or it can take a question and generate an answer.

GPT Transformer will take the input and based on previous data it will generate the output. It is trained on large amounts of text data to learn patterns, grammar, and context, allowing it to perform tasks like translation, summarization, and conversation.

# TO see the how token works (tokenization) -    https://tiktokenizer.vercel.app/